{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4342_HW5_NN",
      "provenance": [],
      "authorship_tag": "ABX9TyOlRRIoogKxUwuFHIQbhkCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykamen/CS4342/blob/main/CS4342_HW5_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_fyOZku4Hq4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzezwg1Z4SsE",
        "outputId": "04847373-3acb-4da1-e780-0df76897e18e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_labels = np.load('drive/MyDrive/fashion_mnist_train_labels.npy', 'r')\n",
        "tr_images = np.load('drive/MyDrive/fashion_mnist_train_images.npy', 'r').T / 255.0\n",
        "te_images = np.load('drive/MyDrive/fashion_mnist_test_images.npy', 'r').T / 255.0\n",
        "te_labels = np.load('drive/MyDrive/fashion_mnist_test_labels.npy', 'r')\n",
        "\n",
        "n_values = np.max(tr_labels) + 1\n",
        "tr_labels = np.eye(n_values)[tr_labels]\n",
        "\n",
        "n_values = np.max(te_labels) + 1\n",
        "te_labels = np.eye(n_values)[te_labels]"
      ],
      "metadata": {
        "id": "1TGv5ZHp4fa0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_INPUT = 784  # Number of input neurons\n",
        "NUM_HIDDEN = 40  # Number of hidden neurons\n",
        "NUM_OUTPUT = 10  # Number of output neurons\n",
        "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
        "\n",
        "# Given a vector w containing all the weights and biased vectors, extract\n",
        "# and return the individual weights and biases W1, b1, W2, b2.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def unpack (w):\n",
        "    # Unpack arguments\n",
        "    start = 0\n",
        "    end = NUM_HIDDEN*NUM_INPUT\n",
        "    W1 = w[0:end]\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN\n",
        "    b1 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT*NUM_HIDDEN\n",
        "    W2 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT\n",
        "    b2 = w[start:end]\n",
        "    # Convert from vectors into matrices\n",
        "    W1 = W1.reshape(NUM_HIDDEN, NUM_INPUT)\n",
        "    W2 = W2.reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
        "    return W1,b1,W2,b2\n",
        "\n",
        "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
        "# return a vector w containing all of them.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def pack (W1, b1, W2, b2):\n",
        "    return np.hstack((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(Z):\n",
        "  yhat = np.exp(Z)\n",
        "  for i in range(Z.shape[0]):\n",
        "    temp = np.sum(yhat[i],axis=0)\n",
        "    yhat[i] = yhat[i]/temp\n",
        "  return yhat.T\n",
        "\n",
        "def pc(yhat, y):\n",
        "  c = 0\n",
        "  for i in range(y.shape[0]):\n",
        "    if np.argmax(yhat[i]) == np.argmax(y[i]):\n",
        "      c = c+1\n",
        "  return c/y.shape[0]\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the cross-entropy (CE) loss, accuracy,\n",
        "# as well as the intermediate values of the NN.\n",
        "def fCE (X, Y, w, a=0.01):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    z1 = W1.dot(X).T + np.tile(b1,(X.shape[1],1))\n",
        "    h1 = relu(z1)\n",
        "    z2 = W2.dot(h1.T).T + np.tile(b2,(X.shape[1],1))\n",
        "    yhat = softmax(z2)\n",
        "    L2 = (1/2)*np.sum(np.square(W2))+(1/2)*np.sum(np.square(W1))\n",
        "    cost = -np.sum(Y.T*np.log(yhat)/(yhat.shape[1]))+a*L2\n",
        "    acc = pc(yhat.T,Y)\n",
        "    return cost, acc, X, z1, h1, W1, W2, yhat\n",
        "\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the gradient of fCE. You might\n",
        "# want to extend this function to return multiple arguments (in which case you\n",
        "# will also need to modify slightly the gradient check code below).\n",
        "def gradCE (X, Y, w, a=0.01):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    cost, acc, X, z1, h1, W1, W2, yhat = fCE(X,Y,w)\n",
        "    temp1 = yhat.T-Y\n",
        "    temp2 = temp1.dot(W2)\n",
        "    g = np.multiply(temp2,reluDerivative(z1)).T\n",
        "    grad_b1 = np.mean(g,axis=1)\n",
        "    grad_b2 = np.mean(yhat.T-Y,axis=0)\n",
        "    grad_W1 = g.dot(X.T)/X.shape[1] + a*W1\n",
        "    grad_W2 = (yhat.T - Y).T.dot(h1)/X.shape[1] + a*W2\n",
        "    return pack(grad_W1, grad_b1, grad_W2, grad_b2)\n",
        "\n",
        "# Given training and testing datasets and an initial set of weights/biases b,\n",
        "# train the NN.\n",
        "def train (trainX, trainY, w, epsilon, alpha, num):\n",
        "  indeces = np.arange(trainX.shape[1])\n",
        "  a = int(trainX.shape[1]/num)\n",
        "  ind = np.split(indeces,a)\n",
        "  for epoch in range(10):\n",
        "    for i in range(a):\n",
        "      gW1,gb1,gW2,gb2 = unpack(gradCE(trainX[:,ind[i]],trainY[ind[i]],w,alpha))\n",
        "      W1,b1,W2,b2 = unpack(w)\n",
        "      W1 = W1-epsilon*gW1\n",
        "      b1 = b1-epsilon*gb1\n",
        "      W2 = W2-epsilon*gW2\n",
        "      b2 = b2-epsilon*gb2\n",
        "      w = pack(W1,b1,W2,b2)\n",
        "  cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX, trainY, w, alpha)\n",
        "  return acc\n",
        "\n",
        "# identical train, but contains the print statement required for this HW.\n",
        "# I was too lazy to separate all of the parts after already finishing the code\n",
        "def train2 (trainX, trainY, w, epsilon, alpha, num):\n",
        "  print(\"For testing number of epochs is increased to (=30). Number of hidden neurons is still (=40).\")\n",
        "  indeces = np.arange(trainX.shape[1])\n",
        "  a = int(trainX.shape[1]/num)\n",
        "  ind = np.split(indeces,a)\n",
        "  for epoch in range(30):\n",
        "    for i in range(a):\n",
        "      gW1,gb1,gW2,gb2 = unpack(gradCE(trainX[:,ind[i]],trainY[ind[i]],w,alpha))\n",
        "      W1,b1,W2,b2 = unpack(w)\n",
        "      W1 = W1-epsilon*gW1\n",
        "      b1 = b1-epsilon*gb1\n",
        "      W2 = W2-epsilon*gW2\n",
        "      b2 = b2-epsilon*gb2\n",
        "      w = pack(W1,b1,W2,b2)\n",
        "      if (epoch == 29 and i >= a-20):\n",
        "        cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX, trainY, w, alpha)\n",
        "        print(f\"For epoch {epoch+1}, batch #{i+1}. Training cost is {cost} and training accuraccy is {acc}.\")\n",
        "  cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX, trainY, w, alpha)\n",
        "  return w\n",
        "\n",
        "def findBestHyperparameters (trainX, trainY):\n",
        "    a = int(trainX.shape[1]*.8)\n",
        "    indeces = np.arange(trainX.shape[1])\n",
        "    np.random.shuffle(indeces)\n",
        "    vl = indeces[a:]\n",
        "    #learning rate = eps, minibatch size = num, regularization strength tested = alpha\n",
        "    pc_best = 0\n",
        "    b_alpha = 0\n",
        "    b_eps = 0\n",
        "    b_num = 0\n",
        "    print(\"Static parameters are number of epochs (=10) and number of hidden neurons (=40).\")\n",
        "    for eps in [.01,.005]:\n",
        "      for alpha in [.01,.005,.001]:\n",
        "        for num in [16,32]:\n",
        "          W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "          b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "          W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "          b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "          w1 = pack(W1, b1, W2, b2)\n",
        "          pc = train(trainX[:,vl],trainY[vl],w1,eps,alpha,num)\n",
        "          print(f\"Current best accuraccy is: {pc_best}. For epsilon: {eps}, alpha: {alpha}, and batch size: {num}, total accuraccy is: {pc}.\")\n",
        "          if(pc > pc_best):\n",
        "            b_alpha = alpha\n",
        "            b_eps = eps\n",
        "            b_num = num\n",
        "            pc_best = pc\n",
        "    return pc_best, b_eps, b_alpha, b_num\n",
        "\n",
        "def test(testX, testY, w, alpha):\n",
        "  cost, acc, X, z1, h1, W1, W2, yhat = fCE(testX, testY, w, alpha)\n",
        "  print(f\"Final accuraccy on the test data set, using optimal hyperparameters, is {acc} and the cross-entropy loss is {cost}\")\n",
        "  return cost, acc"
      ],
      "metadata": {
        "id": "GzJI403O4me-"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "  b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "  W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "  b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "  w1 = pack(W1, b1, W2, b2)"
      ],
      "metadata": {
        "id": "0hebYwrxfMRX"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX,trainY,testX,testY = tr_images,tr_labels,te_images,te_labels"
      ],
      "metadata": {
        "id": "qtY6nIqgc87F"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the gradient is correct on just a few examples (randomly drawn).    \n",
        "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
        "print(\"Numerical gradient:\")\n",
        "print(scipy.optimize.approx_fprime(w1, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], 1e-10))\n",
        "print(\"Analytical gradient:\")\n",
        "print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w1))\n",
        "print(\"Discrepancy:\")\n",
        "print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], \\\n",
        "                                lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_), \\\n",
        "                                w1))"
      ],
      "metadata": {
        "id": "FwfIdTMtc33q",
        "outputId": "acf1c5c4-6c51-4df8-b541-2facda6dd447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical gradient:\n",
            "[ 0.         -0.00015543 -0.00020428 ...  0.08711254  0.10455636\n",
            "  0.08867129]\n",
            "Analytical gradient:\n",
            "[-2.37472291e-06 -1.58756714e-04 -2.07357609e-04 ...  8.71152388e-02\n",
            "  1.04557879e-01  8.86741698e-02]\n",
            "Discrepancy:\n",
            "2.958584840348561e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc_best, b_eps, b_alpha, b_num = findBestHyperparameters (trainX, trainY)"
      ],
      "metadata": {
        "id": "gMcadFqbocZC",
        "outputId": "3b983ee8-fe08-48ab-8128-8a860febe48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static parameters are number of epochs (=10) and number of hidden neurons (=40).\n",
            "Current best accuraccy is: 0. For epsilon: 0.01, alpha: 0.01, and batch size: 16, total accuraccy is: 0.8339166666666666.\n",
            "Current best accuraccy is: 0.8339166666666666. For epsilon: 0.01, alpha: 0.01, and batch size: 32, total accuraccy is: 0.8210833333333334.\n",
            "Current best accuraccy is: 0.8339166666666666. For epsilon: 0.01, alpha: 0.005, and batch size: 16, total accuraccy is: 0.8385833333333333.\n",
            "Current best accuraccy is: 0.8385833333333333. For epsilon: 0.01, alpha: 0.005, and batch size: 32, total accuraccy is: 0.8226666666666667.\n",
            "Current best accuraccy is: 0.8385833333333333. For epsilon: 0.01, alpha: 0.001, and batch size: 16, total accuraccy is: 0.8429166666666666.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.01, alpha: 0.001, and batch size: 32, total accuraccy is: 0.8234166666666667.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.01, and batch size: 16, total accuraccy is: 0.81725.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.01, and batch size: 32, total accuraccy is: 0.7965833333333333.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.005, and batch size: 16, total accuraccy is: 0.82575.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.005, and batch size: 32, total accuraccy is: 0.7950833333333334.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.001, and batch size: 16, total accuraccy is: 0.8245.\n",
            "Current best accuraccy is: 0.8429166666666666. For epsilon: 0.005, alpha: 0.001, and batch size: 32, total accuraccy is: 0.8013333333333333.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(testX, testY, train2(trainX, trainY, w1, b_eps, b_alpha, b_num), b_alpha)"
      ],
      "metadata": {
        "id": "6_EZBYTyULN6",
        "outputId": "9ce2b769-ad3b-48b9-fdd6-f7515a9d6442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For testing number of epochs is increased to (=30). Number of hidden neurons is still (=40).\n",
            "For epoch 30, batch #3731. Training cost is 0.38188530048355035 and training accuraccy is 0.88595.\n",
            "For epoch 30, batch #3732. Training cost is 0.3822263530100157 and training accuraccy is 0.8859333333333334.\n",
            "For epoch 30, batch #3733. Training cost is 0.3804595610066633 and training accuraccy is 0.8868166666666667.\n",
            "For epoch 30, batch #3734. Training cost is 0.3796756093412178 and training accuraccy is 0.8878833333333334.\n",
            "For epoch 30, batch #3735. Training cost is 0.3798354460257199 and training accuraccy is 0.8882666666666666.\n",
            "For epoch 30, batch #3736. Training cost is 0.3717513915756226 and training accuraccy is 0.8907666666666667.\n",
            "For epoch 30, batch #3737. Training cost is 0.37296028512681256 and training accuraccy is 0.8907333333333334.\n",
            "For epoch 30, batch #3738. Training cost is 0.3763783144166961 and training accuraccy is 0.8890333333333333.\n",
            "For epoch 30, batch #3739. Training cost is 0.3761751566730786 and training accuraccy is 0.8888.\n",
            "For epoch 30, batch #3740. Training cost is 0.3728995545889007 and training accuraccy is 0.8903333333333333.\n",
            "For epoch 30, batch #3741. Training cost is 0.37392100716885235 and training accuraccy is 0.8897666666666667.\n",
            "For epoch 30, batch #3742. Training cost is 0.3733908547585356 and training accuraccy is 0.8900166666666667.\n",
            "For epoch 30, batch #3743. Training cost is 0.37513600463961105 and training accuraccy is 0.8888333333333334.\n",
            "For epoch 30, batch #3744. Training cost is 0.3879370115961835 and training accuraccy is 0.8846.\n",
            "For epoch 30, batch #3745. Training cost is 0.3900686409719413 and training accuraccy is 0.88305.\n",
            "For epoch 30, batch #3746. Training cost is 0.3888060630002972 and training accuraccy is 0.884.\n",
            "For epoch 30, batch #3747. Training cost is 0.39479377762468076 and training accuraccy is 0.8815833333333334.\n",
            "For epoch 30, batch #3748. Training cost is 0.392458274942553 and training accuraccy is 0.8826166666666667.\n",
            "For epoch 30, batch #3749. Training cost is 0.3744650320630768 and training accuraccy is 0.8893166666666666.\n",
            "For epoch 30, batch #3750. Training cost is 0.37282973651247303 and training accuraccy is 0.8901333333333333.\n",
            "Final accuraccy on the test data set, using optimal hyperparameters, is 0.8684 and the cross-entropy loss is 0.43601328177847704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.43601328177847704, 0.8684)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    }
  ]
}