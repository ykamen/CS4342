{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4342_HW5_NN",
      "provenance": [],
      "authorship_tag": "ABX9TyO8N/orGHGNK+RMjWL6zLdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykamen/CS4342/blob/main/CS4342_HW5_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y_fyOZku4Hq4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzezwg1Z4SsE",
        "outputId": "cac28101-565a-4e05-ebee-58404771f97f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_labels = np.load('drive/MyDrive/fashion_mnist_train_labels.npy', 'r')\n",
        "tr_images = np.load('drive/MyDrive/fashion_mnist_train_images.npy', 'r').T / 255.0\n",
        "te_images = np.load('drive/MyDrive/fashion_mnist_test_images.npy', 'r').T / 255.0\n",
        "te_labels = np.load('drive/MyDrive/fashion_mnist_test_labels.npy', 'r')\n",
        "\n",
        "n_values = np.max(tr_labels) + 1\n",
        "tr_labels = np.eye(n_values)[tr_labels]\n",
        "\n",
        "n_values = np.max(te_labels) + 1\n",
        "te_labels = np.eye(n_values)[te_labels]"
      ],
      "metadata": {
        "id": "1TGv5ZHp4fa0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_INPUT = 784  # Number of input neurons\n",
        "NUM_HIDDEN = 40  # Number of hidden neurons\n",
        "NUM_OUTPUT = 10  # Number of output neurons\n",
        "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
        "\n",
        "# Given a vector w containing all the weights and biased vectors, extract\n",
        "# and return the individual weights and biases W1, b1, W2, b2.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def unpack (w):\n",
        "    # Unpack arguments\n",
        "    start = 0\n",
        "    end = NUM_HIDDEN*NUM_INPUT\n",
        "    W1 = w[0:end]\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN\n",
        "    b1 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT*NUM_HIDDEN\n",
        "    W2 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT\n",
        "    b2 = w[start:end]\n",
        "    # Convert from vectors into matrices\n",
        "    W1 = W1.reshape(NUM_HIDDEN, NUM_INPUT)\n",
        "    W2 = W2.reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
        "    return W1,b1,W2,b2\n",
        "\n",
        "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
        "# return a vector w containing all of them.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def pack (W1, b1, W2, b2):\n",
        "    return np.hstack((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(Z):\n",
        "  yhat = np.exp(Z)\n",
        "  for i in range(Z.shape[0]):\n",
        "    temp = np.sum(yhat[i],axis=0)\n",
        "    yhat[i] = yhat[i]/temp\n",
        "  return yhat.T\n",
        "\n",
        "def pc(yhat, y):\n",
        "  c = 0\n",
        "  for i in range(y.shape[0]):\n",
        "    if np.argmax(yhat[i]) == np.argmax(y[i]):\n",
        "      c = c+1\n",
        "  return c/y.shape[0]\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the cross-entropy (CE) loss, accuracy,\n",
        "# as well as the intermediate values of the NN.\n",
        "def fCE (X, Y, w):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    z1 = W1.dot(X.T).T + b1 #np.tile(b1,(X.shape[1],1)).T - for multiple units in batch\n",
        "    h1 = relu(z1)\n",
        "    z2 = W2.dot(h1.T).T + b2 #np.tile(b2,(X.shape[1],1)).T â€“ for multiple units in batch\n",
        "    yhat = softmax(z2)\n",
        "    cost = -np.sum(Y.dot(np.log(yhat)))/(yhat.shape[1])\n",
        "    acc = pc(yhat,Y)\n",
        "    return cost, acc, X, z1, h1, W1, W2, yhat\n",
        "\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the gradient of fCE. You might\n",
        "# want to extend this function to return multiple arguments (in which case you\n",
        "# will also need to modify slightly the gradient check code below).\n",
        "def gradCE (X, Y, w):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    cost, acc, X, z1, h1, W1, W2, yhat = fCE(X,Y,w)\n",
        "    temp1 = yhat.T-Y\n",
        "    temp2 = temp1.dot(W2)\n",
        "    g = (temp2*reluDerivative(z1.T).T).T\n",
        "    grad_b1 = g\n",
        "    grad_b2 = (yhat.T-Y)\n",
        "    grad_W1 = g.dot(X)\n",
        "    temp3 = (h1.T).T\n",
        "    grad_W2 = (yhat.T - Y).T.dot(temp3)\n",
        "    return pack(grad_W1, grad_b1, grad_W2, grad_b2)\n",
        "\n",
        "\n",
        "#average of the b1 / b2 and then transform into a row / column vector\n",
        "\n",
        "# Given training and testing datasets and an initial set of weights/biases b,\n",
        "# train the NN.\n",
        "def train (trainX, trainY, testX, testY, w):\n",
        "    batchSize = 256\n",
        "    epsilon = .01\n",
        "    alpha = .01\n",
        "    a = trainX.shape[1] / batchSize\n",
        "    indeces = np.arange(trainX.shape[1])\n",
        "    np.random.shuffle(indeces)\n",
        "    y = trainY\n",
        "    ind = np.split(indeces,a)\n",
        "    for i in range(int(a)):\n",
        "      cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX,trainY,w)\n",
        "      batchy = y[ind[i]]\n",
        "      batchx = trainX[:,ind[i]]\n",
        "      grad_b1, grad_b2, grad_W1, grad_W2 = gradCE(batchx,batchy,w)\n",
        "      W1,b1,W2,b2 = unpack(w)\n",
        "      W1 = W1-epsilon*(grad_W1+alpha*W1/batchSize)\n",
        "      b1 = b1-epsilon*grad_b1\n",
        "      W2 = W2-epsilon*(grad_W2+alpha*W2/batchSize)\n",
        "      b2 = b2-epsilon*grad_b2\n",
        "      w = pack(W1,b1,W2,b2)\n",
        "      if (i >= (int(a)-5)):\n",
        "        cost = -np.sum(trainY*np.log(yhat))/(yhat.shape[0])\n",
        "        acc = pc(yhat,trainY)\n",
        "        print(f\"Loss at batch {i+1} is {cost}\")\n",
        "    return a\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    trainX,trainY,testX,testY = tr_images,tr_labels,te_images,te_labels\n",
        "\n",
        "    # Initialize weights randomly\n",
        "    W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "    W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "    \n",
        "    # Concatenate all the weights and biases into one vector; this is necessary for check_grad\n",
        "    w = pack(W1, b1, W2, b2)\n",
        "\n",
        "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
        "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
        "    print(\"Numerical gradient:\")\n",
        "  #  print(scipy.optimize.approx_fprime(w, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w_)[0], 1e-10))\n",
        "    print(\"Analytical gradient:\")\n",
        "   # print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w))\n",
        "    print(\"Discrepancy:\")\n",
        "    #print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w_), \\\n",
        "     #                               lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[:,idxs]), w_), \\\n",
        "      #                              w))\n",
        "\n",
        "    # Train the network using SGD.\n",
        "  #  train(trainX, trainY, testX, testY, w)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzJI403O4me-",
        "outputId": "bcde219e-91cb-4797-acee-e306778752d5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical gradient:\n",
            "Analytical gradient:\n",
            "Discrepancy:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = 0\n",
        "print(\"Numerical gradient:\")\n",
        "print(scipy.optimize.approx_fprime(w, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], 1e-10))\n",
        "print(\"Analytical gradient:\")\n",
        "print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w))\n",
        "print(\"Discrepancy:\")\n",
        "print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], \\\n",
        "                               lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_), \\\n",
        "                              w))"
      ],
      "metadata": {
        "id": "My71I2U-QXdU",
        "outputId": "5d13cce4-20a7-4628-d482-917d5cf78e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical gradient:\n",
            "[ 0.          0.          0.         ...  0.10310863  0.09741985\n",
            " -0.91501029]\n",
            "Analytical gradient:\n",
            "[ 0.          0.          0.         ...  0.10310648  0.09741803\n",
            " -0.91501308]\n",
            "Discrepancy:\n",
            "1.1153376700730655e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "\n",
        "W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "\n",
        "w = pack(W1, b1, W2, b2)\n",
        "\n",
        "print(trainX.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "#fCE(tr_images,tr_labels,w)\n",
        "#gradCE(tr_images,tr_labels,w)\n",
        "#print(fCE(tr_images,tr_labels,w).shape)\n",
        "train(trainX, trainY, testX, testY, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "gWVsuLGpW3vt",
        "outputId": "ad872fc5-462c-46b2-d882-0d71eb7aefbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "(257, 10)\n",
            "(257, 40)\n",
            "(40, 257)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-28074491dadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#gradCE(tr_images,tr_labels,w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#print(fCE(tr_images,tr_labels,w).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-9faee525e025>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainX, trainY, testX, testY, w)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_W1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_b1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_W2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad_b2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (40,) (40,257) "
          ]
        }
      ]
    }
  ]
}