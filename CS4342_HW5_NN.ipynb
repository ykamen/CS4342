{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4342_HW5_NN",
      "provenance": [],
      "authorship_tag": "ABX9TyMLbgKLgzKRMxgS03Gn3c3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykamen/CS4342/blob/main/CS4342_HW5_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "y_fyOZku4Hq4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzezwg1Z4SsE",
        "outputId": "8e26838b-066e-4814-da8f-9803b23e339b"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_labels = np.load('drive/MyDrive/fashion_mnist_train_labels.npy', 'r')\n",
        "tr_images = np.load('drive/MyDrive/fashion_mnist_train_images.npy', 'r').T / 255.0\n",
        "te_images = np.load('drive/MyDrive/fashion_mnist_test_images.npy', 'r').T / 255.0\n",
        "te_labels = np.load('drive/MyDrive/fashion_mnist_test_labels.npy', 'r')\n",
        "\n",
        "n_values = np.max(tr_labels) + 1\n",
        "tr_labels = np.eye(n_values)[tr_labels]\n",
        "\n",
        "n_values = np.max(te_labels) + 1\n",
        "te_labels = np.eye(n_values)[te_labels]"
      ],
      "metadata": {
        "id": "1TGv5ZHp4fa0"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_INPUT = 784  # Number of input neurons\n",
        "NUM_HIDDEN = 40  # Number of hidden neurons\n",
        "NUM_OUTPUT = 10  # Number of output neurons\n",
        "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
        "\n",
        "# Given a vector w containing all the weights and biased vectors, extract\n",
        "# and return the individual weights and biases W1, b1, W2, b2.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def unpack (w):\n",
        "    # Unpack arguments\n",
        "    start = 0\n",
        "    end = NUM_HIDDEN*NUM_INPUT\n",
        "    W1 = w[0:end]\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN\n",
        "    b1 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT*NUM_HIDDEN\n",
        "    W2 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT\n",
        "    b2 = w[start:end]\n",
        "    # Convert from vectors into matrices\n",
        "    W1 = W1.reshape(NUM_HIDDEN, NUM_INPUT)\n",
        "    W2 = W2.reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
        "    return W1,b1,W2,b2\n",
        "\n",
        "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
        "# return a vector w containing all of them.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def pack (W1, b1, W2, b2):\n",
        "    return np.hstack((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(Z):\n",
        "  yhat = np.exp(Z)\n",
        "  for i in range(Z.shape[0]):\n",
        "    temp = np.sum(yhat[i],axis=0)\n",
        "    yhat[i] = yhat[i]/temp\n",
        "  return yhat.T\n",
        "\n",
        "def pc(yhat, y):\n",
        "  c = 0\n",
        "  for i in range(y.shape[0]):\n",
        "    if np.argmax(yhat[i]) == np.argmax(y[i]):\n",
        "      c = c+1\n",
        "  return c/y.shape[0]\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the cross-entropy (CE) loss, accuracy,\n",
        "# as well as the intermediate values of the NN.\n",
        "def fCE (X, Y, w, a):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    z1 = W1.dot(X).T + np.tile(b1,(X.shape[1],1))\n",
        "    h1 = relu(z1)\n",
        "    z2 = W2.dot(h1.T).T + np.tile(b2,(X.shape[1],1))\n",
        "    yhat = softmax(z2)\n",
        "    L2 = (1/2)*np.sum(np.square(W2))+(1/2)*np.sum(np.square(W1))\n",
        "    cost = -np.sum(Y.T*np.log(yhat))/(yhat.shape[1])+a*L2\n",
        "    acc = pc(yhat,Y)\n",
        "    return cost, acc, X, z1, h1, W1, W2, yhat\n",
        "\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the gradient of fCE. You might\n",
        "# want to extend this function to return multiple arguments (in which case you\n",
        "# will also need to modify slightly the gradient check code below).\n",
        "def gradCE (X, Y, w, a):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    cost, acc, X, z1, h1, W1, W2, yhat = fCE(X,Y,w)\n",
        "    temp1 = yhat.T-Y\n",
        "    temp2 = temp1.dot(W2)\n",
        "    g = np.multiply(temp2,reluDerivative(z1)).T\n",
        "    grad_b1 = np.mean(g,axis=1)\n",
        "    grad_b2 = np.mean(yhat.T-Y,axis=0)\n",
        "    grad_W1 = g.dot(X.T)/X.shape[1] + a*W1\n",
        "    grad_W2 = (yhat.T - Y).T.dot(h1)/X.shape[1] + a*W2\n",
        "    return pack(grad_W1, grad_b1, grad_W2, grad_b2)\n",
        "\n",
        "def findBestHyperparameters (trainX, trainY, w):\n",
        "    W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "    W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "    w = pack(W1, b1, W2, b2)\n",
        "    a = trainX.shape[1]*.8\n",
        "    indeces = np.arange(trainX.shape[1])\n",
        "    ind = np.random.shuffle(indeces)\n",
        "    vlX, vlY = trainX[ind[:a]], trainY[ind[:a]]\n",
        "    trX, trY = trainX[ind[a:]], trainY[ind[a:]]\n",
        "    #learning rate = eps, minibatch size = num, regularization strength tested = alpha\n",
        "    pc_best = 0\n",
        "    for alpha in [.01,.02]:\n",
        "      for eps in [.01,.05]:\n",
        "        for num in [5,10]:\n",
        "          a = trX.shape[1]/num\n",
        "          ind2 = np.split(ind,a)\n",
        "          for i in range(int(a)):\n",
        "\n",
        "\n",
        "# Given training and testing datasets and an initial set of weights/biases b,\n",
        "# train the NN.\n",
        "def train (trainX, trainY, testX, testY, w):\n",
        "    batchSize = 256\n",
        "    epsilon = .01\n",
        "    alpha = .01 #w's penalty\n",
        "    beta = .001 #noise penalty\n",
        "    a = trainX.shape[1] / batchSize\n",
        "    indeces = np.arange(trainX.shape[1])\n",
        "    np.random.shuffle(indeces)\n",
        "    y = trainY\n",
        "    ind = np.split(indeces,a)\n",
        "    for i in range(int(a)):\n",
        "      cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX,trainY,w)\n",
        "      batchy = y[ind[i]]\n",
        "      batchx = trainX[:,ind[i]]\n",
        "      grad_b1, grad_b2, grad_W1, grad_W2 = gradCE(batchx,batchy,w)\n",
        "      W1,b1,W2,b2 = unpack(w)\n",
        "      W1 = W1-epsilon*(grad_W1+alpha*W1/batchSize)\n",
        "      b1 = b1-epsilon*(np.average(grad_b1)+alpha*np.average(grad_b1)/batchSize)\n",
        "      W2 = W2-epsilon*(grad_W2+alpha*W2/batchSize)\n",
        "      b2 = b2-epsilon*(np.average(grad_b2)+alpha*np.average(grad_b2)/batchSize)\n",
        "      w = pack(W1,b1,W2,b2)\n",
        "      if (i >= (int(a)-5)):\n",
        "        cost = -np.sum(trainY*np.log(yhat))/(yhat.shape[0])\n",
        "        acc = pc(yhat,trainY)\n",
        "        print(f\"Loss at batch {i+1} is {cost}\")\n",
        "    return a\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    trainX,trainY,testX,testY = tr_images,tr_labels,te_images,te_labels\n",
        "\n",
        "    # Initialize weights randomly\n",
        "    W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "    W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "    \n",
        "    # Concatenate all the weights and biases into one vector; this is necessary for check_grad\n",
        "    w = pack(W1, b1, W2, b2)\n",
        "\n",
        "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
        "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
        "    print(\"Numerical gradient:\")\n",
        "    print(scipy.optimize.approx_fprime(w, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], 1e-10))\n",
        "    print(\"Analytical gradient:\")\n",
        "    print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w))\n",
        "    print(\"Discrepancy:\")\n",
        "    print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], \\\n",
        "                                    lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_), \\\n",
        "                                    w))\n",
        "\n",
        "    # Train the network using SGD.\n",
        "  #  train(trainX, trainY, testX, testY, w)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzJI403O4me-",
        "outputId": "b2328035-7d1e-46bb-e8d9-df0cab164e30"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[729  73 187 771   1]\n",
            "(784, 5)\n",
            "Numerical gradient:\n",
            "[ 0.01445954  0.01037392  0.03044676 ... -0.09460877  0.10318857\n",
            "  0.10299317]\n",
            "Analytical gradient:\n",
            "[ 0.01443673  0.01037453  0.03042678 ... -0.09461724  0.10318035\n",
            "  0.1029814 ]\n",
            "Discrepancy:\n",
            "1.8340866085317495e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "\n",
        "W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "\n",
        "w = pack(W1, b1, W2, b2)\n",
        "\n",
        "\n",
        "idxs = [0,1]\n",
        "fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w)[0]\n",
        "\n",
        "#gradCE(trainX[:,idxs],trainY[idxs],w)\n",
        "\n",
        "#fCE(tr_images,tr_labels,w)\n",
        "#gradCE(tr_images,tr_labels,w)\n",
        "#print(fCE(tr_images,tr_labels,w).shape)\n",
        "#train(trainX, trainY, testX, testY, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWVsuLGpW3vt",
        "outputId": "0ad57225-9974-4f7d-e7d2-b30b08d2369a"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 40)\n",
            "(2, 40)\n",
            "(2, 10)\n",
            "(10, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.563308718873662"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tr_images.shape)"
      ],
      "metadata": {
        "id": "QfF2U9JXPDqg",
        "outputId": "36921a7d-6f3c-4575-d343-dee0508ac977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 60000)\n"
          ]
        }
      ]
    }
  ]
}