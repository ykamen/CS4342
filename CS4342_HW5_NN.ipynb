{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4342_HW5_NN",
      "provenance": [],
      "authorship_tag": "ABX9TyMKMdqZG5AlXh11YkWptTlP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykamen/CS4342/blob/main/CS4342_HW5_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_fyOZku4Hq4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import scipy.optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzezwg1Z4SsE",
        "outputId": "0ea967e8-a50b-40ad-f3ff-68aadd646063"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_labels = np.load('drive/MyDrive/fashion_mnist_train_labels.npy', 'r')\n",
        "tr_images = np.load('drive/MyDrive/fashion_mnist_train_images.npy', 'r').T / 255.0\n",
        "te_images = np.load('drive/MyDrive/fashion_mnist_test_images.npy', 'r').T / 255.0\n",
        "te_labels = np.load('drive/MyDrive/fashion_mnist_test_labels.npy', 'r')\n",
        "\n",
        "n_values = np.max(tr_labels) + 1\n",
        "tr_labels = np.eye(n_values)[tr_labels]\n",
        "\n",
        "n_values = np.max(te_labels) + 1\n",
        "te_labels = np.eye(n_values)[te_labels]"
      ],
      "metadata": {
        "id": "1TGv5ZHp4fa0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_INPUT = 784  # Number of input neurons\n",
        "NUM_HIDDEN = 40  # Number of hidden neurons\n",
        "NUM_OUTPUT = 10  # Number of output neurons\n",
        "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
        "\n",
        "# Given a vector w containing all the weights and biased vectors, extract\n",
        "# and return the individual weights and biases W1, b1, W2, b2.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def unpack (w):\n",
        "    # Unpack arguments\n",
        "    start = 0\n",
        "    end = NUM_HIDDEN*NUM_INPUT\n",
        "    W1 = w[0:end]\n",
        "    start = end\n",
        "    end = end + NUM_HIDDEN\n",
        "    b1 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT*NUM_HIDDEN\n",
        "    W2 = w[start:end]\n",
        "    start = end\n",
        "    end = end + NUM_OUTPUT\n",
        "    b2 = w[start:end]\n",
        "    # Convert from vectors into matrices\n",
        "    W1 = W1.reshape(NUM_HIDDEN, NUM_INPUT)\n",
        "    W2 = W2.reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
        "    return W1,b1,W2,b2\n",
        "\n",
        "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
        "# return a vector w containing all of them.\n",
        "# This is useful for performing a gradient check with check_grad.\n",
        "def pack (W1, b1, W2, b2):\n",
        "    return np.hstack((W1.flatten(), b1.flatten(), W2.flatten(), b2.flatten()))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(Z):\n",
        "  yhat = np.exp(Z)\n",
        "  for i in range(Z.shape[0]):\n",
        "    temp = np.sum(yhat[i],axis=0)\n",
        "    yhat[i] = yhat[i]/temp\n",
        "  return yhat.T\n",
        "\n",
        "def pc(yhat, y):\n",
        "  c = 0\n",
        "  for i in range(y.shape[0]):\n",
        "    if np.argmax(yhat[i]) == np.argmax(y[i]):\n",
        "      c = c+1\n",
        "  return c/y.shape[0]\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the cross-entropy (CE) loss, accuracy,\n",
        "# as well as the intermediate values of the NN.\n",
        "def fCE (X, Y, w):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    z1 = W1.dot(X).T + np.tile(b1,(X.shape[1],1))\n",
        "    h1 = relu(z1)\n",
        "    z2 = W2.dot(h1.T).T + np.tile(b2,(X.shape[1],1))\n",
        "    yhat = softmax(z2)\n",
        "    cost = -np.sum(Y.dot(np.log(yhat)))/(yhat.shape[1])\n",
        "    acc = pc(yhat,Y)\n",
        "    return cost, acc, X, z1, h1, W1, W2, yhat\n",
        "\n",
        "\n",
        "def reluDerivative(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "# Given training images X, associated labels Y, and a vector of combined weights\n",
        "# and bias terms w, compute and return the gradient of fCE. You might\n",
        "# want to extend this function to return multiple arguments (in which case you\n",
        "# will also need to modify slightly the gradient check code below).\n",
        "def gradCE (X, Y, w):\n",
        "    W1, b1, W2, b2 = unpack(w)\n",
        "    cost, acc, X, z1, h1, W1, W2, yhat = fCE(X,Y,w)\n",
        "    temp1 = yhat.T-Y\n",
        "    temp2 = temp1.dot(W2)\n",
        "    g = (temp2*reluDerivative(z1)).T\n",
        "    grad_b1 = np.average(g,axis=1)\n",
        "    grad_b2 = np.average(yhat.T-Y,axis=1)\n",
        "    grad_W1 = g.dot(X.T)\n",
        "    grad_W2 = (yhat.T - Y).T.dot(h1)\n",
        "    return pack(grad_W1, grad_b1, grad_W2, grad_b2)\n",
        "\n",
        "\n",
        "#average of the b1 / b2 and then transform into a row / column vector\n",
        "\n",
        "# Given training and testing datasets and an initial set of weights/biases b,\n",
        "# train the NN.\n",
        "def train (trainX, trainY, testX, testY, w):\n",
        "    batchSize = 256\n",
        "    epsilon = .01\n",
        "    alpha = .01 #w's penalty\n",
        "    beta = .001 #noise penalty\n",
        "    a = trainX.shape[1] / batchSize\n",
        "    indeces = np.arange(trainX.shape[1])\n",
        "    np.random.shuffle(indeces)\n",
        "    y = trainY\n",
        "    ind = np.split(indeces,a)\n",
        "    for i in range(int(a)):\n",
        "      cost, acc, X, z1, h1, W1, W2, yhat = fCE(trainX,trainY,w)\n",
        "      batchy = y[ind[i]]\n",
        "      batchx = trainX[:,ind[i]]\n",
        "      grad_b1, grad_b2, grad_W1, grad_W2 = gradCE(batchx,batchy,w)\n",
        "      W1,b1,W2,b2 = unpack(w)\n",
        "      W1 = W1-epsilon*(grad_W1+alpha*W1/batchSize)\n",
        "      b1 = b1-epsilon*(np.average(grad_b1)+alpha*np.average(grad_b1)/batchSize)\n",
        "      W2 = W2-epsilon*(grad_W2+alpha*W2/batchSize)\n",
        "      b2 = b2-epsilon*(np.average(grad_b2)+alpha*np.average(grad_b2)/batchSize)\n",
        "      w = pack(W1,b1,W2,b2)\n",
        "      if (i >= (int(a)-5)):\n",
        "        cost = -np.sum(trainY*np.log(yhat))/(yhat.shape[0])\n",
        "        acc = pc(yhat,trainY)\n",
        "        print(f\"Loss at batch {i+1} is {cost}\")\n",
        "    return a\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    trainX,trainY,testX,testY = tr_images,tr_labels,te_images,te_labels\n",
        "\n",
        "    # Initialize weights randomly\n",
        "    W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "    W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "    \n",
        "    # Concatenate all the weights and biases into one vector; this is necessary for check_grad\n",
        "    w = pack(W1, b1, W2, b2)\n",
        "\n",
        "    # Check that the gradient is correct on just a few examples (randomly drawn).\n",
        "    idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
        "    print(idxs)\n",
        "    print(trainX[:,idxs].shape)\n",
        "    print(\"Numerical gradient:\")\n",
        "    #print(scipy.optimize.approx_fprime(w, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], 1e-10))\n",
        "    print(\"Analytical gradient:\")\n",
        "    #print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w))\n",
        "    print(\"Discrepancy:\")\n",
        "    #print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], \\\n",
        "     #                               lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_), \\\n",
        "      #                              w))\n",
        "\n",
        "    # Train the network using SGD.\n",
        "  #  train(trainX, trainY, testX, testY, w)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzJI403O4me-",
        "outputId": "65abd12b-cc5a-4c4a-f125-1dc410e3d96f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114 140 136  77 499]\n",
            "(784, 5)\n",
            "Numerical gradient:\n",
            "Analytical gradient:\n",
            "Discrepancy:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = [0,1]\n",
        "print(trainX[:,idxs].shape)\n",
        "print(\"Numerical gradient:\")\n",
        "print(scipy.optimize.approx_fprime(w, lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], 1e-10))\n",
        "print(\"Analytical gradient:\")\n",
        "print(gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w))\n",
        "print(\"Discrepancy:\")\n",
        "print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_)[0], \\\n",
        "                               lambda w_: gradCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w_), \\\n",
        "                              w))"
      ],
      "metadata": {
        "id": "My71I2U-QXdU",
        "outputId": "43a61e77-bd37-4338-b5bd-bae3fdf4f1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 2)\n",
            "Numerical gradient:\n",
            "[ 0.          0.          0.         ...  0.21684876  0.17131185\n",
            " -0.79809048]\n",
            "Analytical gradient:\n",
            "[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            " 1.11022302e-17 1.52655666e-17]\n",
            "Discrepancy:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f2015508404e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Discrepancy:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                \u001b[0;32mlambda\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgradCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mcheck_grad\u001b[0;34m(func, grad, x0, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                          (list(kwargs.keys()),))\n\u001b[1;32m    813\u001b[0m     return sqrt(sum((grad(x0, *args) -\n\u001b[0;32m--> 814\u001b[0;31m                      approx_fprime(x0, func, step, *args))**2))\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (31802,) (31810,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_INPUT))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
        "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
        "\n",
        "W2 = 2*(np.random.random(size=(NUM_OUTPUT, NUM_HIDDEN))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
        "b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
        "\n",
        "w = pack(W1, b1, W2, b2)\n",
        "\n",
        "\n",
        "idxs = [0,1]\n",
        "fCE(np.atleast_2d(trainX[:,idxs]), np.atleast_2d(trainY[idxs]), w)[0]\n",
        "\n",
        "\n",
        "#fCE(tr_images,tr_labels,w)\n",
        "#gradCE(tr_images,tr_labels,w)\n",
        "#print(fCE(tr_images,tr_labels,w).shape)\n",
        "#train(trainX, trainY, testX, testY, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWVsuLGpW3vt",
        "outputId": "a5524cf8-650a-407e-c948-d2a374e7930e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "(2, 10)\n"
          ]
        }
      ]
    }
  ]
}